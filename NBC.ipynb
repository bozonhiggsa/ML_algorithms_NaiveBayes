{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eEoj1DOwZsmM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DtUaewx1ZsmP"
   },
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "    df_for_tests = df.head()    \n",
    "    \n",
    "    idx = np.arange(df.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_set_size = int(df.shape[0] * 0.8)\n",
    "\n",
    "    train_set = df.loc[idx[:train_set_size]]\n",
    "    test_set = df.loc[idx[train_set_size:]]\n",
    "    \n",
    "    return train_set, test_set, df_for_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ae4YTbLtZsmR"
   },
   "outputs": [],
   "source": [
    "train_set, test_set, df_for_tests = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>spam</td>\n",
       "      <td>500 New Mobiles from 2004, MUST GO! Txt: NOKIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh... Lk tt den we take e one tt ends at cine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thats cool! Sometimes slow and gentle. Sonetim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>ham</td>\n",
       "      <td>what number do u live at? Is it 11?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>spam</td>\n",
       "      <td>GSOH? Good with SPAM the ladies?U could b a ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "1374  spam  500 New Mobiles from 2004, MUST GO! Txt: NOKIA...\n",
       "2417   ham  Oh... Lk tt den we take e one tt ends at cine ...\n",
       "650    ham  Thats cool! Sometimes slow and gentle. Sonetim...\n",
       "3326   ham                what number do u live at? Is it 11?\n",
       "4100  spam  GSOH? Good with SPAM the ladies?U could b a ma..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data analysis\n",
    "\n",
    "train_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OZDyrOMZZsmV"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "\n",
    "def clean_data(message):\n",
    "    \n",
    "    \"\"\" \n",
    "    Returns string which consists of message words\n",
    "    \n",
    "    Argument:\n",
    "    message -- message from dataset; \n",
    "        type(message) -> <class 'str'>\n",
    "    \n",
    "    Returns:\n",
    "    result -- cleaned message, which contains only letters a-z, and numbers 0-9, with only one space between words;\n",
    "        type(clean_data(message)) -> <class 'str'>\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    # заменяем в строке все одиночные символы (не представленные в паттерне) на пустую строку\n",
    "    # т.е. удаляем всё кроме латинских букв, цифр и пробелов    \n",
    "    \n",
    "    message_temp1 = re.sub(r'[^a-zA-Z0-9\\s]', '', message)\n",
    "    \n",
    "    # заменяем один или несколько пробелов подряд на один пробел\n",
    "    message_temp2 = re.sub(r'(?:\\s+)', ' ', message_temp1)\n",
    "    \n",
    "    message_temp3 = message_temp2.lower()\n",
    "    \n",
    "    return message_temp3.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "r96K1BnlZsmX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned:  does not operate 667 after lt gt or what\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Does, not  \\\\operate 66.7 after & lt;# & gt; or what'\n",
    "print('cleaned: ',clean_data(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# преобразование колонки датафрейма (серии) в список\n",
    "list(train_set.v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does', 'not', 'operate', '667', 'after', 'lt', 'gt', 'or', 'what']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.split возвращает список подстрок (слов)\n",
    "\n",
    "re.split(r'\\s', 'does not operate 667 after lt gt or what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gEFD2wuFZsma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457,)\n",
      "(1115,)\n",
      "(4457,)\n",
      "(1115,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list(['it', 'will', 'stop', 'on', 'itself', 'i', 'however', 'suggest', 'she', 'stays', 'with', 'someone', 'that', 'will', 'be', 'able', 'to', 'give', 'ors', 'for', 'every', 'stool']),\n",
       "       list(['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', '2', 'make', 'contact', 'with', 'ufind', 'out', 'who', 'they', 'rreveal', 'who', 'thinks', 'ur', 'so', 'specialcall', 'on', '09058094599']),\n",
       "       list(['im', 'home']), ...,\n",
       "       list(['thats', 'cool', 'sometimes', 'slow', 'and', 'gentle', 'sonetimes', 'rough', 'and', 'hard']),\n",
       "       list(['what', 'number', 'do', 'u', 'live', 'at', 'is', 'it', '11']),\n",
       "       list(['gsoh', 'good', 'with', 'spam', 'the', 'ladiesu', 'could', 'b', 'a', 'male', 'gigolo', '2', 'join', 'the', 'uks', 'fastest', 'growing', 'mens', 'club', 'reply', 'oncall', 'mjzgroup', '087143423992stop', 'reply', 'stop', 'msg150rcvd'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparation data for model\n",
    "\n",
    "def prep_for_model(train_set, test_set):\n",
    "    \n",
    "    \"\"\" \n",
    "    Returns arrays of train/test features(words) and train/test targets(labels)\n",
    "    \n",
    "    Arguments:\n",
    "    train_set -- train dataset, which consists of train messages and labels; \n",
    "        type(train_set) -> pandas.core.frame.DataFrame\n",
    "    test_set -- test dataset, which consists of test messages and labels; \n",
    "        type(train_set) -> pandas.core.frame.DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    train_set_x -- array which contains lists of words of each cleaned train message; \n",
    "        (type(train_set_x) ->numpy.ndarray[list[str]], train_set_x.shape = (num_messages,))\n",
    "    train_set_y -- array of train labels (names of classes), \n",
    "        (type(train_set_y) -> numpy.ndarray, train_set_y.shape = (num_messages,))\n",
    "    test_set_x -- array which contains lists of words of each cleaned test message;\n",
    "        (type(test_set_x) numpy.ndarray[list[str]], test_set_x.shape = (num_messages,)\n",
    "    test_set_y -- array of test labels (names of classes), \n",
    "        (type(test_set_y) -> numpy.ndarray, test_set_y.shape = (num_messages,))\n",
    "        \"\"\"\n",
    "    \n",
    "   \n",
    "    # преобразуем колонку датафрейма в массив Numpy\n",
    "    train_set_y = np.array(list(train_set.v1))\n",
    "    print(train_set_y.shape)\n",
    "    test_set_y = np.array(list(test_set.v1))\n",
    "    print(test_set_y.shape)\n",
    "    \n",
    "    # взять каждую строку из списка, очистить её и разбить на подстроки (разделитель - пробел),\n",
    "    # т.е. создать список подстрок (слов), полученные списки слов добавить в список train_x_list\n",
    "    train_x_list = []\n",
    "    for str in list(train_set.v2):\n",
    "        train_x_list.append(re.split(r'\\s', clean_data(str)))    \n",
    "        \n",
    "    # взять каждую строку из списка, очистить её и разбить на подстроки (разделитель - пробел),\n",
    "    # т.е. создать список подстрок (слов), полученные списки слов добавить в список test_x_list\n",
    "    test_x_list = []    \n",
    "    for str in list(test_set.v2):\n",
    "        test_x_list.append(re.split(r'\\s', clean_data(str)))\n",
    "        \n",
    "    # на базе списка списков создать массивы Numpy\n",
    "    # поскольку вложенные списки имеют разную длину, то увеличения размерности массива не происходит,\n",
    "    # отдельные вложенные списки трактуются как отдельные объекты и поэтому\n",
    "    # получаем одномерный массив Numpy\n",
    "    train_set_x = np.array(train_x_list)\n",
    "    print(train_set_x.shape)\n",
    "    test_set_x = np.array(test_x_list) \n",
    "    print(test_set_x.shape)\n",
    "        \n",
    "    return train_set_x, train_set_y, test_set_x, test_set_y\n",
    "\n",
    "train_set_x, train_set_y, test_set_x, test_set_y = prep_for_model(train_set, test_set)\n",
    "train_set_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['convey', 'my', 'regards', 'to', 'him']),\n",
       "       list(['jay', 'wants', 'to', 'work', 'out', 'first', 'hows', '4', 'sound']),\n",
       "       list(['wait', '4', 'me', 'in', 'sch', 'i', 'finish', 'ard', '5']),\n",
       "       ...,\n",
       "       list(['hi', 'do', 'u', 'want', 'to', 'join', 'me', 'with', 'sts', 'later', 'meeting', 'them', 'at', 'five', 'call', 'u', 'after', 'class']),\n",
       "       list(['hey', 'sorry', 'i', 'didntgive', 'ya', 'a', 'a', 'bellearlier', 'hunny']),\n",
       "       list(['go', 'fool', 'dont', 'cheat', 'others', 'ok'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bnNLInEBZsmc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(2,)\n",
      "(3,)\n",
      "(2,)\n",
      "ham:['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
      "ham:['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n"
     ]
    }
   ],
   "source": [
    "a1, a2, b1, b2 = prep_for_model(df_for_tests.head(3), df_for_tests.tail(2))\n",
    "print('{}:{}'.format(a2[0], a1[0]))\n",
    "print('{}:{}'.format(b2[0], b1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds_ham = np.where(a2 == 'ham')\n",
    "inds_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds_ham = np.where(a2 == 'ham')[0]\n",
    "inds_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds_spam = np.where(a2 == 'spam')[0]\n",
    "inds_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "olXobIliZsmg"
   },
   "outputs": [],
   "source": [
    "# Check words in categories\n",
    "\n",
    "def categories_words(x_train, y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns arrays of features(words) in each category and in both categories\n",
    "    \n",
    "    Arguments:\n",
    "    x_train -- array which contains lists of words of each cleaned train message; \n",
    "        (type(x_train) -> numpy.ndarray[list[str]], x_train.shape = (num_messages,))\n",
    "    \n",
    "    Returns:\n",
    "    all_words_list -- array of all words in both categories;\n",
    "        (type(all_words_list) -> numpy.ndarray[str], all_words_list.shape = (num_words,))\n",
    "    ham_words_list -- array of words in 'ham' class;\n",
    "        (type(ham_words_list) -> numpy.ndarray[str], ham_words_list.shape = (num_words,))\n",
    "    spam_words_list -- array of words in 'spam' class;\n",
    "        (type(spam_words_list) -> numpy.ndarray[str], spam_words_list.shape = (num_words,))        \n",
    "    \"\"\"\n",
    "    all_words_list = []\n",
    "    ham_words_list = []\n",
    "    spam_words_list = []    \n",
    "    \n",
    "    # берём каждый список из массива x_train, слова из каждого списка добавляем в список all_words_list\n",
    "    for list_str in x_train:\n",
    "        all_words_list.extend(list_str)\n",
    "    \n",
    "    # получаем массив индексов строк датасета, соответствующих классу ham\n",
    "    inds_ham = np.where(y_train == 'ham')[0]\n",
    "    \n",
    "    # берём из массива x_train каждый список, соответствующий классу ham,\n",
    "    # слова из каждого такого списка добавляем в список ham_words_list\n",
    "    for ind in inds_ham:\n",
    "        ham_words_list.extend(x_train[ind])\n",
    "        \n",
    "    # получаем массив индексов строк датасета, соответствующих классу spam\n",
    "    inds_spam = np.where(y_train == 'spam')[0]\n",
    "    \n",
    "    # берём из массива x_train каждый список, соответствующий классу spam,\n",
    "    # слова из каждого такого списка добавляем в список spam_words_list\n",
    "    for ind in inds_spam:\n",
    "        spam_words_list.extend(x_train[ind])    \n",
    "    \n",
    "    return np.array(all_words_list), np.array(ham_words_list), np.array(spam_words_list)\n",
    "\n",
    "all_words_list_a1, ham_words_list_a1, spam_words_list_a1 = categories_words(a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zCkJB-7TZsmj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five \"ham\" words of a1:  ['go' 'until' 'jurong' 'point' 'crazy']\n"
     ]
    }
   ],
   "source": [
    "print('first five \"ham\" words of a1: ', ham_words_list_a1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [1, 2, 3, 1, 4, 5, 1, 6, 7]\n",
    "list1.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1.count(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LO7_8FEKZsmm"
   },
   "outputs": [],
   "source": [
    "class Naive_Bayes(object):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha: int\n",
    "        The smoothing coeficient.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.train_set_x = None\n",
    "        self.train_set_y = None\n",
    "        \n",
    "        self.all_words_list = []\n",
    "        self.ham_words_list = []\n",
    "        self.spam_words_list = []\n",
    "    \n",
    "    def fit(self, train_set_x, train_set_y):\n",
    "        \n",
    "        # Generate all_words_list, ham_words_list, spam_words_list using function 'categories_words'; \n",
    "        # Calculate probability of each word in both categories        \n",
    "        self.train_set_x = train_set_x\n",
    "        self.train_set_y = train_set_y\n",
    "        \n",
    "        self.all_words_list, self.ham_words_list, self.spam_words_list = categories_words(train_set_x, train_set_y)\n",
    "        \n",
    "        count_ham = 0\n",
    "        count_spam = 0\n",
    "        # определяем общее количество экземпляров для класса ham и для класса spam\n",
    "        for class_word in train_set_y:\n",
    "            if class_word == 'ham':\n",
    "                count_ham += 1\n",
    "            elif class_word == 'spam':\n",
    "                count_spam += 1                \n",
    "                \n",
    "        # сохраняем количество экземпляров каждого из классов в обучающем датасете\n",
    "        self.number_ham = count_ham\n",
    "        print(f'Number of examples of ham = {self.number_ham}')\n",
    "        self.number_spam = count_spam\n",
    "        print(f'Number of examples of spam = {self.number_spam}')\n",
    "        \n",
    "        # вычисляем и сохраняем априорные вероятности классов        \n",
    "        self.P_ham = np.log(count_ham / len(train_set_y))\n",
    "        print(f'P_ham = {np.exp(self.P_ham)}')\n",
    "        self.P_spam = np.log(count_spam / len(train_set_y))\n",
    "        print(f'P_spam = {np.exp(self.P_spam)}')\n",
    "        \n",
    "        # преобразуем массивы обратно в списки, чтобы иметь возможность вызвать метод count()\n",
    "        list_ham_words_list = list(self.ham_words_list)\n",
    "        list_spam_words_list = list(self.spam_words_list)\n",
    "        # создаём словари, которые будут хранить частотность слов\n",
    "        # в качестве ключей будем использовать сами слова\n",
    "        # в словаре dict_count_word_ham будут ключи, соответствующие всем возможным словам\n",
    "        # в экземплярах ham класса\n",
    "        # в словаре dict_count_word_spam будут ключи, соответствующие всем возможным словам\n",
    "        # в экземплярах spam класса\n",
    "        self.dict_count_word_ham = {}\n",
    "        self.dict_count_word_spam = {}\n",
    "        \n",
    "        for word in self.ham_words_list:\n",
    "            if word not in self.dict_count_word_ham:\n",
    "                count_in_ham = list_ham_words_list.count(word)\n",
    "                self.dict_count_word_ham[word] = count_in_ham\n",
    "        for word in self.spam_words_list:                \n",
    "            if word not in self.dict_count_word_spam:\n",
    "                count_in_spam = list_spam_words_list.count(word)\n",
    "                self.dict_count_word_spam[word] = count_in_spam    \n",
    "        \n",
    "        \n",
    "    def predict(self, test_set_x):\n",
    "        \n",
    "        # Return list of predicted labels for test set; type(prediction) -> list, len(prediction) = len(test_set_y)\n",
    "        prediction = []        \n",
    "                \n",
    "        # test_set_x - это массив списков слов, один список слов для каждого тестового экземпляра\n",
    "        # задача - отнести каждый список слов к одному из классов\n",
    "        for list_words in test_set_x:            \n",
    "            P_likelyhood_ham = 0\n",
    "            P_likelyhood_spam = 0\n",
    "            for word in list_words:                              \n",
    "                # проверяем наличие частотности каждого слова из списка слов тестового экземпляра\n",
    "                # в словаре dict_count_word_ham. Если в словаре нет такого слова - \n",
    "                # устанавливаем для него нулевую частотность, если есть - берём частотность из словаря\n",
    "                if word not in self.dict_count_word_ham:\n",
    "                    count_in_ham = 0\n",
    "                else:\n",
    "                    count_in_ham = self.dict_count_word_ham[word]\n",
    "                # проверяем наличие частотности каждого слова из списка слов тестового экземпляра\n",
    "                # в словаре dict_count_word_spam. Если в словаре нет такого слова - \n",
    "                # устанавливаем для него нулевую частотность, если есть - берём частотность из словаря\n",
    "                if word not in self.dict_count_word_spam:\n",
    "                    count_in_spam = 0\n",
    "                else:\n",
    "                    count_in_spam = self.dict_count_word_spam[word]\n",
    "                \n",
    "                # count_in_ham - сколько раз данное слово word встречается в экземплярах обучающего\n",
    "                # датасета, относящихся к классу ham\n",
    "                # count_in_spam - сколько раз данное слово word встречается в экземплярах обучающего\n",
    "                # датасета, относящихся к классу spam\n",
    "                \n",
    "                # количество слов (признаков) в данном тестовом экземпляре\n",
    "                number_of_features = len(list_words)                \n",
    "                \n",
    "                count_all = count_in_ham + count_in_spam\n",
    "                N_ham = count_all\n",
    "                N_spam = count_all\n",
    "                \n",
    "                theta_ham = (count_in_ham + self.alpha) / (N_ham + self.alpha * number_of_features)\n",
    "                theta_spam = (count_in_spam + self.alpha) / (N_spam + self.alpha * number_of_features)                                      \n",
    "                \n",
    "                P_likelyhood_ham += np.log(theta_ham)                \n",
    "                P_likelyhood_spam += np.log(theta_spam)            \n",
    "            \n",
    "            if (self.P_ham + P_likelyhood_ham) > (self.P_spam + P_likelyhood_spam):\n",
    "                prediction.append('ham')\n",
    "            else:\n",
    "                prediction.append('spam')            \n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "two70VtTZsmp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples of ham = 3851\n",
      "Number of examples of spam = 606\n",
      "P_ham = 0.8640341036571685\n",
      "P_spam = 0.1359658963428315\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "model = Naive_Bayes(alpha=a)\n",
    "model.fit(train_set_x, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0-zUCJU_Zsmy"
   },
   "outputs": [],
   "source": [
    "y_predictions = model.predict(test_set_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KBFDdUdkZsm0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9417040358744395\n"
     ]
    }
   ],
   "source": [
    "actual = list(test_set_y)\n",
    "accuracy = (y_predictions == test_set_y).mean()\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sklearn for multinomial distribution of data\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 10))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 0, 1, 3, 0, 0, 1, 4, 4],\n",
       "       [1, 2, 4, 2, 4, 3, 4, 2, 4, 2],\n",
       "       [4, 1, 1, 0, 1, 1, 1, 1, 0, 4],\n",
       "       [1, 0, 0, 3, 2, 1, 0, 3, 1, 1],\n",
       "       [3, 4, 0, 1, 3, 4, 2, 4, 0, 3],\n",
       "       [1, 2, 0, 4, 1, 2, 2, 1, 0, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "(150,)\n",
      "(150, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x1   x2   x3   x4    y\n",
       "145  6.7  3.0  5.2  2.3  2.0\n",
       "146  6.3  2.5  5.0  1.9  2.0\n",
       "147  6.5  3.0  5.2  2.0  2.0\n",
       "148  6.2  3.4  5.4  2.3  2.0\n",
       "149  5.9  3.0  5.1  1.8  2.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sklearn for normal distribution of data\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "print(type(y))\n",
    "print(y.shape)\n",
    "yy = y.reshape(len(y), -1)\n",
    "print(yy.shape)\n",
    "df = pd.DataFrame(np.c_[X, yy], columns=['x1', 'x2', 'x3', 'x4', 'y'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "NBC.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
